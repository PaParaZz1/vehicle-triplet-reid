#!/usr/bin/env python3
from argparse import ArgumentParser, FileType
from importlib import import_module
from itertools import count
import os

import h5py
import json
import numpy as np
from sklearn.metrics import average_precision_score
import tensorflow as tf

import common
import loss


parser = ArgumentParser(description='Evaluate a ReID embedding.')

parser.add_argument(
    '--excluder', required=True, choices=('market1501', 'diagonal'),
    help='Excluder function to mask certain matches. Especially for multi-'
         'camera datasets, one often excludes pictures of the query person from'
         ' the gallery if it is taken from the same camera. The `diagonal`'
         ' excluder should be used if this is *not* required.')

parser.add_argument(
    '--query_dataset', required=True,
    help='Path to the query dataset csv file.')

parser.add_argument(
    '--query_embeddings', required=True,
    help='Path to the h5 file containing the query embeddings.')

parser.add_argument(
    '--gallery_dataset', required=True,
    help='Path to the gallery dataset csv file.')

parser.add_argument(
    '--gallery_embeddings', required=True,
    help='Path to the h5 file containing the query embeddings.')

parser.add_argument(
    '--metric', required=True, choices=loss.cdist.supported_metrics,
    help='Which metric to use for the distance between embeddings.')

parser.add_argument(
    '--filename', type=FileType('w'),
    help='Optional name of the json file to store the results in.')

parser.add_argument(
    '--batch_size', default=256, type=common.positive_int,
    help='Batch size used during evaluation, adapt based on your memory usage.')

parser.add_argument(
    '--reranking_type', default='combine', choices=['combine', 'topk'])

parser.add_argument(
    '--reranking_topk', default=50, type=common.positive_int)

config = tf.ConfigProto()
config.gpu_options.allow_growth = True


def main():
    # Verify that parameters are set correctly.
    args = parser.parse_args()

    # Load the query and gallery data from the CSV files.
    query_pids, query_fids = common.load_dataset(args.query_dataset, None)
    gallery_pids, gallery_fids = common.load_dataset(args.gallery_dataset, None)

    root_path = os.path.dirname(args.query_embeddings)
    query_emb_slt = os.path.basename(args.query_embeddings).split('_')
    gallery_emb_slt = os.path.basename(args.gallery_embeddings).split('_')
    query_sup_emb = os.path.join(root_path, '{}_sup_{}'.format('_'.join(query_emb_slt[:-1]), query_emb_slt[-1]))
    query_rl_emb = os.path.join(root_path, '{}_rl_{}'.format('_'.join(query_emb_slt[:-1]), query_emb_slt[-1]))
    gallery_sup_emb = os.path.join(root_path, '{}_sup_{}'.format('_'.join(gallery_emb_slt[:-1]), gallery_emb_slt[-1]))
    gallery_rl_emb = os.path.join(root_path, '{}_rl_{}'.format('_'.join(gallery_emb_slt[:-1]), gallery_emb_slt[-1]))
    # Load the two datasets fully into memory.
    # load embs generated by re-id network
    with h5py.File(query_sup_emb, 'r') as f_query:
        query_sup_embs = np.array(f_query['emb'])
    with h5py.File(gallery_sup_emb, 'r') as f_gallery:
        gallery_sup_embs = np.array(f_gallery['emb'])

    # load embs generated by rl network
    with h5py.File(query_rl_emb, 'r') as f_query:
        query_rl_embs = np.array(f_query['emb'])
    with h5py.File(gallery_rl_emb, 'r') as f_gallery:
        gallery_rl_embs = np.array(f_gallery['emb'])

    # Just a quick sanity check that both have the same embedding dimension!
    query_dim = query_sup_embs.shape[1]
    gallery_dim = gallery_sup_embs.shape[1]
    if query_dim != gallery_dim:
        raise ValueError('Shape mismatch between query ({}) and gallery ({}) '
                         'dimension'.format(query_dim, gallery_dim))

    # Setup the dataset specific matching function
    excluder = import_module('excluders.' + args.excluder).Excluder(gallery_fids)

    # We go through the queries in batches, but we always need the whole gallery
    batch_pids, batch_fids, batch_sup_embs = tf.data.Dataset.from_tensor_slices(
        (query_pids, query_fids, query_sup_embs)
    ).batch(args.batch_size).make_one_shot_iterator().get_next()

    batch_pids, batch_fids, batch_rl_embs = tf.data.Dataset.from_tensor_slices(
        (query_pids, query_fids, query_rl_embs)
    ).batch(args.batch_size).make_one_shot_iterator().get_next()

    batch_sup_distances = loss.cdist(batch_sup_embs, gallery_sup_embs, metric=args.metric)
    batch_rl_distances = loss.cdist(batch_rl_embs, gallery_rl_embs, metric=args.metric)

    # Loop over the query embeddings and compute their APs and the CMC curve.
    aps = []
    cmc = np.zeros(len(gallery_pids), dtype=np.int32)
    with tf.Session(config=config) as sess:
        for start_idx in count(step=args.batch_size):
            try:
                # Compute distance to all gallery embeddings
                rl_distances, sup_distances, pids, fids = sess.run([
                    batch_rl_distances, batch_sup_distances, batch_pids, batch_fids])
                if args.reranking_type == 'combine':
                    _lbd = 0.55
                    distances = _lbd * rl_distances + (1 - _lbd) * sup_distances
                    # distances = rl_distances + sup_distances
                    # distances = rl_distances
                    # distances = sup_distances
                    # distances = (rl_distances / np.max(rl_distances)) + (sup_distances / np.max(sup_distances))
                    # distances = np.maximum(sup_distances - rl_distances, 0)
                elif args.reranking_type == 'topk':
                    distances = sup_distances
                    for bth_idx in range(len(distances)):
                        _lbd = 0.8
                        topk = sup_distances[bth_idx].argsort()[:args.reranking_topk]
                        others = sup_distances[bth_idx].argsort()[args.reranking_topk:]
                        # distances[bth_idx][topk] = rl_distances[bth_idx][topk]
                        distances[bth_idx][topk] = (1 - _lbd) * sup_distances[bth_idx][topk] + _lbd * rl_distances[bth_idx][topk]
                        # distances[bth_idx][topk] = sup_distances[bth_idx][topk] + rl_distances[bth_idx][topk]
                        # distances[bth_idx][topk] = np.minimum(sup_distances[bth_idx][topk], rl_distances[bth_idx][topk])
                        distances[bth_idx][topk] = (sup_distances[bth_idx][topk] / np.max(sup_distances[bth_idx][topk])) + (rl_distances[bth_idx][topk] / np.max(rl_distances[bth_idx][topk]))
                        # distances[bth_idx][topk] = np.maximum(sup_distances[bth_idx][topk] - rl_distances[bth_idx][topk], 0)
                        distances[bth_idx][others] = np.inf
                print('\rEvaluating batch {}-{}/{}'.format(
                        start_idx, start_idx + len(fids), len(query_fids)),
                      flush=True, end='')
            except tf.errors.OutOfRangeError:
                print()  # Done!
                break

            # Convert the array of objects back to array of strings
            pids, fids = np.array(pids, '|U'), np.array(fids, '|U')

            # Compute the pid matches
            pid_matches = gallery_pids[None] == pids[:,None]

            # Get a mask indicating True for those gallery entries that should
            # be ignored for whatever reason (same camera, junk, ...) and
            # exclude those in a way that doesn't affect CMC and mAP.
            mask = excluder(fids)
            distances[mask] = np.inf
            pid_matches[mask] = False

            # Keep track of statistics. Invert distances to scores using any
            # arbitrary inversion, as long as it's monotonic and well-behaved,
            # it won't change anything.
            scores = 1 / (1 + distances)
            for i in range(len(distances)):
                ap = average_precision_score(pid_matches[i], scores[i])

                if np.isnan(ap):
                    print()
                    print("WARNING: encountered an AP of NaN!")
                    print("This usually means a person only appears once.")
                    print("In this case, it's because of {}.".format(fids[i]))
                    print("I'm excluding this person from eval and carrying on.")
                    print()
                    continue

                aps.append(ap)
                # Find the first true match and increment the cmc data from there on.
                k = np.where(pid_matches[i, np.argsort(distances[i])])[0][0]
                cmc[k:] += 1

    # Compute the actual cmc and mAP values
    cmc = cmc / len(query_pids)
    mean_ap = np.mean(aps)

    # Save important data
    if args.filename is not None:
        json.dump({'mAP': mean_ap, 'CMC': list(cmc), 'aps': list(aps)}, args.filename)

    # Print out a short summary.
    print('mAP: {:.2%} | top-1: {:.2%} top-2: {:.2%} | top-5: {:.2%} | top-10: {:.2%}'.format(
        mean_ap, cmc[0], cmc[1], cmc[4], cmc[9]))

if __name__ == '__main__':
    main()
